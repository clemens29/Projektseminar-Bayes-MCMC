% Latex document @author Clemens Näther, Jakub Kliemann, s85426, s85515
%--------------------------------------------------------------------
\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{pdfpages}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{mhchem}

\usepackage{biblatex} % Verwende BibLaTeX
\addbibresource{bibliography.bib} % Deine .bib-Datei

\lstset{ %
  language=,
  basicstyle=\ttfamily\small,
  backgroundcolor=\color{lightgray!20},
  frame=single,
  columns=fullflexible,
  breaklines=true,
  captionpos=b
}

\geometry{a4paper, top=25mm, left=30mm, right=25mm, bottom=30mm,
headsep=10mm, footskip=12mm}

\pagestyle{fancy}

\lhead{Dokumentation}

\rhead{Seite \thepage\ von \pageref{LastPage}}

\cfoot{}

\renewcommand{\headrulewidth}{0.4pt}

\renewcommand{\footrulewidth}{0.4pt}

\begin{document}

\begin{titlepage}

\begin{center}

\includegraphics[height=4cm]{../images/htwd-logo.jpg}\\[1cm]

\textsc{\LARGE Hochschule für Technik und Wirtschaft}\\[1.5cm]

\textsc{\Large Dokumentation}\\[0.5cm]

% Title
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\HRule \\[0.4cm]
{ \huge \bfseries \textsc{Projektseminar}}\\[0.4cm]
{ \huge \bfseries \textsc{Optimierung und Unsicherheitsquantifizierung mit Bayesianischer Statistik und MCMC-Methoden}}\\[0.4cm]
{ \huge \bfseries \textsc{(Prof. Schwarzenberger)}}\\[0.4cm]
\HRule \\[1.5cm]

% Author and supervisor
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large

\emph{Clemens Näther, s85426}\\
\emph{Jakub Kliemann, s85515}\\

\end{flushleft}
\end{minipage}
\end{center}
\end{titlepage}

\tableofcontents
\newpage

\section{Einleitung}
\newpage

\section{Theoretischer Teil}

\subsection{Grundlagen der bayesianischen Statistik und das Bayes'sche Theorem}
\subsubsection{Einführung in die bayesianische Statistik}

Die bayesianische Statistik ist ein Zweig der Statistik. Sie unterscheidet sich im wesentlichen in der Interpretation der Wahrscheinlichkeit von der klassischen Statistik. Die klassische Statistik definiert die Wahrscheinlichkeit als die \textbf{relative Häufigkeit} in einem Zufallsexperiment \parencite[2]{StatistikKlassischOderBayes}. In der bayesianischen Statistik hingegen wird die Wahrscheinlichkeit als Grad des Glaubens respektiv als \textbf{Plausibilität} eines Ereignisses oder einer Aussage interpretiert \parencite[1]{EinfBayesStatistik}. \\\\
Kern der bayesianischen Statistik ist es Wissen über ein Ereignis zu verfeinern, sobald neue Informationen vorliegen. Dazu nutzt man hauptsächlich das \textbf{Bayes'sche Theorem}, welches erlaubt das Vorwissen (Prior) mit neuen Daten (Likelihood) zu kombinieren und daraus eine aktualisierte Wahrscheinlichkeit (Posterior) zu berechnen. \\\\
Mit Hilfe des Bayes'schen Theorems kann man unbekannte Parameter schätzen, ein Konfidenzintervall für diese Parameter angeben und Hypothesen prüfen. Die klassische Statistik benötigt hingegen dafür Testgrößen, weshalb die bayesianische Statistik als flexibler und intuitiver gilt. \parencite[1]{EinfBayesStatistik}. \\\\
Problem der bayesianischen Statistik ist jedoch, dass die Berechnung der Posterioriverteilung analytisch oft nicht möglich ist. Da es nun aber gute numerische Methoden wie die \textbf{Markov Chain Monte Carlo (MCMC)} Methoden gibt, findet die bayesianische Statistik immer mehr Anwendungen. So zum Beispiel in der Medizin oder für künstliche Intelligenzen. \parencite[1]{StatistikKlassischOderBayes}.

\subsubsection{Das Bayes'sche Theorem und seine Bestandteile}

Das Bayes'sche Theorem ist ein fundamentales Konzept der bayesianischen Statistik.Es beschreibt, wie man vorhandenes Vorwissen durch neue Daten aktualisiert. \\\\
Die \textbf{Prioriverteilung} beschreibt die anfänglichen Annahmen oder das Vorwissen über einen 
Parameter oder ein Ereignis, bevor neue Daten berücksichtigt werden.
Dabei ``enthält die Priorverteilung eines Parameters $\theta$, ausgedrückt durch f ($ \theta $), 
was man vor Auswertung der Stichprobe über $\theta$ weiß.'' \parencite[90]{StatistikKlassischOderBayes}. \\
Als Priori-Wahrscheinlichkeit wird somit die Wahrscheinlichkeit $ P(A) $ bezeichnet. \\\\
Die \textbf{Posterioriverteilung} beschreibt das Wissen über einen Parameter oder ein Ereignis, nachdem alle 
vorhandenen Daten berücksichtigt wurden. Durch die neuen Daten, meist einer Stichprobe, wird die 
anfängliche Annahme, die durch die Prioriverteilung ausgedrückt wird, aktualisiert. 
Dies führt zu einer neuen Verteilung die widerspiegelt, wie wahrscheinlich verschiedene Werte 
des Parameters auf Grundlage sowohl des Vorwissens als auch der neuen Informationen sind. \parencite[109]{StatistikKlassischOderBayes}\\
Die Posteriori-Wahrscheinlichkeit wird somit als $ P(A|B) $ bezeichnet. \\\\
Die \textbf{Likelihood-Funktion} enthält die Informationen, die die Daten über den Parameter oder das Ereignis liefern.
Dabei beschreibt die Likelihood die Informationen aus den neuen Daten, die zur Aktualisierung der Prioriverteilung beitragen. \parencite[88]{StatistikKlassischOderBayes}\\
Die Likelihood-Wahrscheinlichkeit wird somit als $ P(B|A) $ bezeichnet. \\\\
Die Wahrscheinlichkeit $ P(B) $ wird als Normierungskonstante bezeichnet. Sie sorgt dafür, 
dass die Posterioriverteilung korrekt normiert ist, das heißt, dass die Summe der 
Wahrscheinlichkeiten aller möglichen Werte des Parameters 1 ergibt. \parencite[109]{StatistikKlassischOderBayes} \\\\
Das Bayes'sche Theorem lässt sich somit wie folgt darstellen:
\begin{equation}
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\end{equation}
Alternativ kann das Bayes-Theorem ohne die Normierungskonstante $ P(B) $ in folgender proportionaler Form dargestellt werden \parencite[15]{BayesStatistik}:
\begin{equation}
P(A|B) \propto P(B|A) \cdot P(A)
\end{equation} 


\subsubsection{Beispiele und praktische Anwendungen}
\textbf{Beispiel 1}: $m$ gleichgeformte Kugeln, unter denen sich $k$ rote Kugeln und $m-k$ schwarze Kugeln befinden. Eine Kugel wird zufällig gezogen. 
Die Wahrscheinlichkeit, dass die gezogene Kugel rot ist, beträgt 
\begin{equation}
P(A) = \frac{k}{m} = p
\end{equation}
Der Versuch wird erweitert, sodass $n$-mal eine Kugel mit Zurücklegen gezogen wird.
Die Wahrscheinlichkeit, dass $x$-mal eine rote Kugel bie $n$-maligem Ziehen gezogen wird,
beträgt
\begin{equation}
P(x|n,p) = \binom{n}{x} \cdot p^x \cdot (1-p)^{n-x}
\end{equation}
Sei nun $p$ unbekannt. Dieses $p$ ist nun zu schätzen.
Die Binomialverteilung wird nun als Likelihood-Funktion verwendet:
\begin{equation}
P(n,x|p) = \binom{n}{x} \cdot p^x \cdot (1-p)^{n-x}
\end{equation}
wobei $0 \leq p \geq 1$.
Als Prioridichte wird die Gleichverteilung verwendet, da es keine Informationen über $p$ gibt.
\begin{equation}
  P(p) =
  \begin{cases}
    1, & \text{für } 0 \leq p \leq 1 \\
    0, & \text{sonst}
  \end{cases}
\end{equation}
Die Posterioridichte ergibt sich somit zu:
\begin{equation}
P(p|n,x) = \frac{\binom{n}{x}  p^x \cdot (1-p)^{n-x}}{P(n,x)}
\label{eq:posterior}
\end{equation}
Vergleicht man dies mit der Dichtefunktion der Beta-Verteilung, so erkennt man dass die Posterioridichte einer Beta-Verteilung entspricht.
\begin{equation}
P(p|n,x) = \frac{(n+1)!}{x!\cdot(n-x)!} \cdot p^x \cdot (1-p)^{n-x}
= \frac{\Gamma(n+1)}{\Gamma(x+1)\cdot\Gamma(n-x+1)} \cdot p^x \cdot (1-p)^{n-x}
\end{equation}
Somit suche nach Maximum der Posterioridichte, um den Schätzer für $p$ zu finden.
\begin{equation}
\frac{d}{dp} P(p|n,x) = xp^{x-1} \cdot (1-p)^{n-x} - (n-x)p^x \cdot (1-p)^{n-x-1} = 0
\end{equation}
\begin{equation}
\Rightarrow xp^{x-1} \cdot (1-p)^{n-x} = (n-x)p^x \cdot (1-p)^{n-x-1}
\end{equation}
Vereinfacht ergibt sich:
\begin{equation}
\Rightarrow x(1-p) = (n-x)p
\end{equation}
\begin{equation}  
\Rightarrow \frac{x}{p} - \frac{n-x}{1-p} = 0
\end{equation}
\begin{equation}
\Rightarrow p = \frac{x}{n}
\end{equation}
Der Schätzer für $p$ ist somit der relative Anteil der roten Kugeln an der Gesamtanzahl der Kugeln. \\\\
\textbf{Beispiel 2}: Beispiel 1 wird erweitert. Es wird nun eine zweite Stichprobe gezogen. 
Stichprobe 1: $n_1 = 10$, $x_1 = 4$, Stichprobe 2: $n_2 = 20$, $x_2 = 6$.
Daten sind unabhängig voneinander. Die Daten (Posterioriverteilung) der 1. Stichprobe dienen nun
als Prioridichte für die 2. Stichprobe. Man erhält somit:
\begin{equation}
P(p|n_1,x_1,n_2,x_2) = \frac{P(p|n_1,x_1) \cdot P(p|n_2,x_2)}{P(n_1,x_1,n_2,x_2)}
\end{equation}
Dabei ist die Prioridichte identisch zur Posterioridichte der 1. Stichprobe, siehe Gleichung \eqref{eq:posterior}.
Die Posterioridichte der 2. Stichprobe ergibt sich somit zu:
\begin{equation}
P(p|n_1,x_1,n_2,x_2) = \frac{\binom{n_1+n_2}{x_1+x_2} \cdot p^{x_1} \cdot (1-p)^{n_1-x_1} \cdot p^{x_2} \cdot (1-p)^{n_2-x_2}}{P(n_1,x_1,n_2,x_2)}
\end{equation}
Oder mithilfe der Beta-Verteilung:
\begin{equation}
  P(p|n_1,x_1,n_2,x_2) = \frac{\Gamma(n_1+n_2+1)}{\Gamma(x_1+x_2+1)\cdot\Gamma(n_1+n_2-x_1-x_2+1)} \cdot p^{x_1+x_2} \cdot (1-p)^{n_1+n_2-x_1-x_2}
\end{equation}
Die Daten der 1. und 2. Stichprobe könnnen somit kombiniert werden.
Für die Daten $n_1 = 10$, $x_1 = 4$, $n_2 = 20$, $x_2 = 6$:
\begin{equation}
P(p|10,4,20,6) = 931 395 465p^{10} \cdot (1-p)^{20}
\end{equation}



\newpage

\subsection{Binomiale Verteilung und deren bayesianische Interpretation}
\newpage

\subsection{Markov Chain Monte Carlo (MCMC) Methoden}
\newpage

\subsection{Konvergenzkriterien und Diagnosewerkzeuge für MCMC-Simulationen}
\newpage

\section{Praktischer Teil}

\subsection{Implementierung bayesianischer Modelle unter Verwendung in Python}
\newpage

\subsection{Anwendung der Modelle auf verschiedene Datensätze}
\newpage

\subsection{Durchführung von MCMC-Simulationen}
\newpage

\subsection{Interpretation der Ergebnisse}
\newpage

\subsection{Vergleich mit klassischen Methoden}
\newpage

\section{Zusammenfassung und Ausblick}
\newpage


\section{Literaturverzeichnis} 

\printbibliography 
\newpage

\section{Selbstständigkeitserklärung}

\end{document}