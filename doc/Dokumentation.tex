% Latex document @author Clemens Näther, Jakub Kliemann, s85426, s85515
%--------------------------------------------------------------------
\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{pdfpages}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{mhchem}
\usepackage{subcaption}
\usepackage{comment}
\usepackage[german]{babel}
\renewcommand{\subsectionautorefname}{Abschnitt}

\usepackage{biblatex} % Verwende BibLaTeX
\addbibresource{bibliography.bib} % Deine .bib-Datei

\lstset{ %
  language=,
  basicstyle=\ttfamily\small,
  backgroundcolor=\color{lightgray!20},
  frame=single,
  columns=fullflexible,
  breaklines=true,
  captionpos=b
}

\lstset{ 
  language=Python, 
  basicstyle=\ttfamily, 
  numbers=left, 
  frame=single, 
  breaklines=true, 
  aboveskip=10pt,   
  xleftmargin=20pt, 
  belowskip=20pt    
}

\geometry{a4paper, top=25mm, left=30mm, right=25mm, bottom=30mm,
headsep=10mm, footskip=12mm}

\pagestyle{fancy}

\lhead{Dokumentation}

\rhead{Seite \thepage\ von \pageref{LastPage}}

\cfoot{}

\renewcommand{\headrulewidth}{0.4pt}

\renewcommand{\footrulewidth}{0.4pt}

\begin{document}

\begin{titlepage}

\begin{center}

\includegraphics[height=4cm]{../images/htwd_logo.jpg}\\[1cm]

\textsc{\LARGE Hochschule für Technik und Wirtschaft}\\[1.5cm]

\textsc{\Large Dokumentation}\\[0.5cm]

% Title
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\HRule \\[0.4cm]
{ \huge \bfseries \textsc{Projektseminar}}\\[0.4cm]
{ \huge \bfseries \textsc{Optimierung und Unsicherheitsquantifizierung mit Bayesianischer Statistik und MCMC-Methoden}}\\[0.4cm]
{ \huge \bfseries \textsc{(Prof. Schwarzenberger)}}\\[0.4cm]
\HRule \\[1.5cm]

% Author and supervisor
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large

\emph{Clemens Näther, s85426}\\
\emph{Jakub Kliemann, s85515}\\

\end{flushleft}
\end{minipage}
\end{center}
\end{titlepage}

\tableofcontents
\newpage

\section{Einleitung}
Die Statistik ist ein wesentlicher Bestandteil der Wissenschaft. Sie ermöglicht es, aus Daten Informationen zu gewinnen und aus diesen Schlussfolgerungen zu ziehen. Es exsistieren dabei aber zwei verschiedene Ansätze, die klassische und die bayesianische Statistik. Beide bieten unterschiedliche Perspektiven und Werkzeuge zur Analyse und Interpretation von Unsicherheiten, und die Entscheidung für eine Methode kann erhebliche Auswirkungen auf die Schlussfolgerungen einer Studie haben.\\\\
In dieser Arbeit liegt der Fokus auf der bayesianischen Statistik, einem Ansatz, der die Wahrscheinlichkeit als subjektives Maß für die Unsicherheit interpretiert und es ermöglicht, Vorwissen systematisch in den Analyseprozess einzubringen. Der Vergleich mit der klassischen Statistik, die auf der relativen Häufigkeit basiert, zeigt, wie unterschiedlich die Ansätze sowohl in der Theorie als auch in der Praxis sind.\\\\
Ein zentraler Schwerpunkt der Arbeit liegt darin die theoretischen Grundlagen der bayesianischen Statistik zu erläutern und den Prinzipien der klassischen Statistik gegen-überzustellen. Dabei soll insbesondere auf die Markov Chain Monte Carlo (MCMC) Methoden eingegangen werden, die es ermöglichen, komplexe Modelle zu schätzen und zu simulieren. Inbesondere wird der Metropolis-Hastings-Algorithmus, einer der bekanntesten Algorithmen zur Erstellung von Markovketten vorgestellt.\\\\
Anschließend wird im zweiten Teil der Arbeit die praktische Anwendung der bayesianischen Statistik und der MCMC-Methoden anhand von Beispielen und Datensätzen in Python demonstriert. Dabei wird gezeigt, wie bayesianische Modelle implementiert und auf verschiedene Datensätze angewendet werden können. Die Ergebnisse werden interpretiert und mit klassischen Methoden verglichen.\\\\
\newpage

\section{Theoretischer Teil}

\subsection{Grundlagen der bayesianischen Statistik und das Bayes'sche Theorem}
\subsubsection{Einführung in die bayesianische Statistik}

Die bayesianische Statistik ist ein Zweig der Statistik. Sie unterscheidet sich im wesentlichen in der Interpretation der Wahrscheinlichkeit von der klassischen Statistik. Die klassische Statistik definiert die Wahrscheinlichkeit als die \textbf{relative Häufigkeit} in einem Zufallsexperiment \parencite[2]{StatistikKlassischOderBayes}. In der bayesianischen Statistik hingegen wird die Wahrscheinlichkeit als Grad des Glaubens respektiv als \textbf{Plausibilität} eines Ereignisses oder einer Aussage interpretiert \parencite[1]{EinfBayesStatistik}. \\\\
Kern der bayesianischen Statistik ist es Wissen über ein Ereignis zu verfeinern, sobald neue Informationen vorliegen. Dazu nutzt man hauptsächlich das \textbf{Bayes'sche Theorem}, welches erlaubt das Vorwissen (Prior) mit neuen Daten (Likelihood) zu kombinieren und daraus eine aktualisierte Wahrscheinlichkeit (Posterior) zu berechnen. \\\\
Mit Hilfe des Bayes'schen Theorems kann man unbekannte Parameter schätzen, ein Konfidenzintervall für diese Parameter angeben und Hypothesen prüfen. Die klassi-sche Statistik benötigt hingegen dafür Testgrößen, weshalb die bayesianische Statistik als flexibler und intuitiver gilt. \parencite[1]{EinfBayesStatistik}. \\\\
Problem der bayesianischen Statistik ist jedoch, dass die Berechnung der Posterioriverteilung analytisch oft nicht möglich ist. Da es nun aber gute numerische Methoden wie die \textbf{Markov Chain Monte Carlo (MCMC)} Methoden gibt, findet die bayes-ianische Statistik immer mehr Anwendungen. So zum Beispiel in der Medizin oder für künstliche Intelligenzen. \parencite[1]{StatistikKlassischOderBayes}.

\subsubsection{Das Bayes'sche Theorem und seine Bestandteile}

Das Bayes'sche Theorem ist ein fundamentales Konzept der bayesianischen Statistik. Es beschreibt, wie man vorhandenes Vorwissen durch neue Daten aktualisiert. \\\\
Die \textbf{Prioriverteilung} beschreibt die anfänglichen Annahmen oder das Vorwissen über einen 
Parameter oder ein Ereignis, bevor neue Daten berücksichtigt werden.
Dabei ``enthält die Priorverteilung eines Parameters $\theta$, ausgedrückt durch $f (\theta) $, 
was man vor Auswertung der Stichprobe über $\theta$ weiß.'' \parencite[90]{StatistikKlassischOderBayes}. \\
Als Priori-Wahrscheinlichkeit wird somit die Wahrscheinlichkeit $ P(A) $ bezeichnet. \\\\
Die \textbf{Posterioriverteilung} beschreibt das Wissen über einen Parameter oder ein Ereignis, nachdem alle 
vorhandenen Daten berücksichtigt wurden. Durch die neuen Daten, meist einer Stichprobe, wird die 
anfängliche Annahme, die durch die Prioriverteilung ausgedrückt wird, aktualisiert. 
Dies führt zu einer neuen Verteilung die widerspiegelt, wie wahrscheinlich verschiedene Werte 
des Parameters auf Grundlage sowohl des Vorwissens als auch der neuen Informationen sind. \parencite[109]{StatistikKlassischOderBayes}\\
Die Posteriori-Wahrscheinlichkeit wird somit als $ P(A|B) $ bezeichnet. \\\\
Die \textbf{Likelihood-Funktion} enthält die Informationen, die die Daten über den Parameter oder das Ereignis liefern.
Dabei beschreibt die Likelihood die Informationen aus den neuen Daten, die zur Aktualisierung der Prioriverteilung beitragen. \parencite[88]{StatistikKlassischOderBayes}\\
Die Likelihood-Wahrscheinlichkeit wird somit als $ P(B|A) $ bezeichnet. \\\\
Die Wahrscheinlichkeit $ P(B) $ wird als Normierungskonstante bezeichnet. Sie sorgt dafür, 
dass die Posterioriverteilung korrekt normiert ist, das heißt, dass die Summe der 
Wahrscheinlichkeiten aller möglichen Werte des Parameters 1 ergibt. \parencite[109]{StatistikKlassischOderBayes} \\\\
Das Bayes'sche Theorem lässt sich somit wie folgt darstellen:
\begin{equation}
P(A|B) = \frac{P(A) \cdot P(B|A)}{P(B)}
\end{equation}
Das Bayes'sche Theorem lässt sich auch rekursiv anwenden \parencite[17]{EinfBayesStatistik}. \\
Gegeben sei das Ereignis $A$ sowie die Teilergebnisse $B_1, B_2, ..., B_n$. Dann ergibt sich die Wahrscheinlichkeit $P(A|B_1)$ zu:
\begin{equation}
P(A|B_1) = \frac{P(A) \cdot P(B_1|A)}{P(B_1)}
\end{equation}
Nun wird die Information $B_2$ hinzugefügt. Die Wahrscheinlichkeit $P(A|B_1, B_2)$ ergibt sich bei Unabhängigkeit von den Teilereignissen $B_1, B_2, ..., B_n$ zu:
\begin{equation}
P(A|B_1, B_2) = \frac{P(A) \cdot P(B_1|A) \cdot P(B_2|A)}{P(B_1) \cdot P(B_2)}
\end{equation}
Weiterhin lässt sich diese Formel umstellen, wodurch deutlich wird, dass beim Hinzufügen von neuen Informationen die Posterioriverteilung aktualisiert wird:
\begin{equation}
P(A|B_1, B_2) = \frac{P(A) \cdot P(B_1|A) \cdot P(B_2|A)}{P(B_1) \cdot P(B_2)} = P(A|B_1) \cdot \frac{P(B_2|A)}{P(B_2)}
\end{equation}
Dies lässt sich allgemein formulieren für:
\begin{equation}
P(A|B_1, B_2, ..., B_n) = P(A|B_1, B_2, ..., B_{n-1}) \cdot \frac{P(B_n|A)}{P(B_n)}
\end{equation}
Die Wahl der Prioriverteilung ist ein wichtiger Aspekt der bayesianischen Statistik. Sie wird immer so gewählt, dass die Entropie maximal ist. Die Entropie ist ein Maß für die Unsicherheit, was bedeutet, dass nur Informationen enthalten sind, die vor der Beobachtung bekannt sind. \parencite[57]{EinfBayesStatistik}. Unter folgenden Bedingungen ist die Prioriverteilung optimal \parencite[59]{EinfBayesStatistik}:
\begin{itemize}
  \item Zufallsvariablen, die in $[a,b]$ definiert sind, sind \textbf{gleichverteilt}
  \item Zufallsvariablen mit gegebenen Mittelwert und Varianz sind \textbf{normalverteilt}
  \item Zufallsvariablen mit gegebenen Mittelwert sind \textbf{exponentialverteilt}
  \item Zufallsvariablen mit gegebenen Mittelwert und Varianz im Intervall [0,$\infty$] besitzen eine \textbf{abgeschnittene Normalverteilung}
\end{itemize}
Wenn keine Informationen über den Parameter vorliegen, wird eine \textbf{uninformative} Prioriverteilung gewählt. Es handelt sich dabei um eine uneigentliche Verteilung. \parencite[57]{EinfBayesStatistik}.

\subsubsection{Beispiele und praktische Anwendungen}
\textbf{Beispiel 1}: $m$ gleichgeformte Kugeln, unter denen sich $k$ rote Kugeln und $m-k$ schwarze Kugeln befinden. Eine Kugel wird zufällig gezogen. 
Die Wahrscheinlichkeit, dass die gezogene Kugel rot ist, beträgt 
\begin{equation}
P(A) = \frac{k}{m} = p
\end{equation}
Der Versuch wird erweitert, sodass $n$-mal eine Kugel mit Zurücklegen gezogen wird.
Die Wahrscheinlichkeit, dass $x$-mal eine rote Kugel bie $n$-maligem Ziehen gezogen wird,
beträgt
\begin{equation}
P(x|n,p) = \binom{n}{x} \cdot p^x \cdot (1-p)^{n-x}
\end{equation}
Sei nun $p$ unbekannt. Dieses $p$ ist nun zu schätzen.
Die Binomialverteilung wird nun als Likelihood-Funktion verwendet:
\begin{equation}
P(n,x|p) = \binom{n}{x} \cdot p^x \cdot (1-p)^{n-x}
\end{equation}
wobei $0 \leq p \leq 1$.
Als Prioridichte wird die Gleichverteilung verwendet, da es keine Informationen über $p$ gibt.
\begin{equation}
  P(p) =
  \begin{cases}
    1, & \text{für } 0 \leq p \leq 1 \\
    0, & \text{sonst}
  \end{cases}
\end{equation}
Die Posterioridichte ergibt sich somit zu:
\begin{equation}
P(p|n,x) = \frac{\binom{n}{x}  p^x \cdot (1-p)^{n-x}}{P(n,x)}
\label{eq:posterior}
\end{equation}
Vergleicht man dies mit der Dichtefunktion der Beta-Verteilung, so erkennt man dass die Posterioridichte einer Beta-Verteilung entspricht.
\begin{equation}
P(p|n,x) = \frac{(n+1)!}{x!\cdot(n-x)!} \cdot p^x \cdot (1-p)^{n-x}
= \frac{\Gamma(n+1)}{\Gamma(x+1)\cdot\Gamma(n-x+1)} \cdot p^x \cdot (1-p)^{n-x}
\end{equation}
Somit suche nach Maximum der Posterioridichte, um den Schätzer für $p$ zu finden.
\begin{equation}
\frac{d}{dp} P(p|n,x) = xp^{x-1} \cdot (1-p)^{n-x} - (n-x)p^x \cdot (1-p)^{n-x-1} = 0
\end{equation}
\begin{equation}
\Rightarrow xp^{x-1} \cdot (1-p)^{n-x} = (n-x)p^x \cdot (1-p)^{n-x-1}
\end{equation}
Vereinfacht ergibt sich:
\begin{equation}
\Rightarrow x(1-p) = (n-x)p
\end{equation}
\begin{equation}  
\Rightarrow \frac{x}{p} - \frac{n-x}{1-p} = 0
\end{equation}
\begin{equation}
\Rightarrow p = \frac{x}{n}
\end{equation}
Der Schätzer für $p$ ist somit der relative Anteil der roten Kugeln an der Gesamtanzahl der Kugeln. \\\\
\textbf{Beispiel 2}: Beispiel 1 wird erweitert. Es wird nun eine zweite Stichprobe gezogen. 
Stichprobe 1: $n_1 = 10$, $x_1 = 4$, Stichprobe 2: $n_2 = 20$, $x_2 = 6$.
Daten sind unabhängig voneinander. Die Daten (Posterioriverteilung) der 1. Stichprobe dienen nun
als Prioridichte für die 2. Stichprobe. Man erhält somit:
\begin{equation}
P(p|n_1,x_1,n_2,x_2) = \frac{P(p|n_1,x_1) \cdot P(p|n_2,x_2)}{P(n_1,x_1,n_2,x_2)}
\end{equation}
Dabei ist die Prioridichte identisch zur Posterioridichte der 1. Stichprobe, siehe Gleichung \eqref{eq:posterior}.
Die Posterioridichte der 2. Stichprobe ergibt sich somit zu:
\begin{equation}
P(p|n_1,x_1,n_2,x_2) = \frac{\binom{n_1+n_2}{x_1+x_2} \cdot p^{x_1} \cdot (1-p)^{n_1-x_1} \cdot p^{x_2} \cdot (1-p)^{n_2-x_2}}{P(n_1,x_1,n_2,x_2)}
\end{equation}
Oder mithilfe der Beta-Verteilung:
\begin{equation}
  P(p|n_1,x_1,n_2,x_2) = \frac{\Gamma(n_1+n_2+1)}{\Gamma(x_1+x_2+1)\cdot\Gamma(n_1+n_2-x_1-x_2+1)} \cdot p^{x_1+x_2} \cdot (1-p)^{n_1+n_2-x_1-x_2}
\end{equation}
Die Daten der 1. und 2. Stichprobe könnnen somit kombiniert werden.
Für die Daten $n_1 = 10$, $x_1 = 4$, $n_2 = 20$, $x_2 = 6$:
\begin{equation}
P(p|10,4,20,6) = 931 395 465p^{10} \cdot (1-p)^{20}
\end{equation}

\subsubsection{Punktschätzer, Konfidenzintervalle und Hypothesenprüfung in der bayesianischen Statistik}
\paragraph{Punktschätzer} \mbox{}\\\\
Im folgenden wird die Schätzung eines Parameters mithilfe der Bayes-Strategie erläutert. \\
Die möglichen Schätzwerte der Parameter $x$ werden als $\hat{x}$ bezeichnet. Die wahren Parameter werden als $x$ bezeichnet. \\
Es wird eine Kostenfunktion $L(\hat{x},x)$ definiert, die die Kosten für die Schätzung $\hat{x}$ des wahren Parameters $x$ angibt.
Dies bedeutet, dass die Kostenfunktion die Differenz zwischen dem wahren Parameter $x$ und der Schätzung $\hat{x}$ angibt.
Dabei gibt es verschiedene Kostenfunktionen, die verwendet werden können. \parencite[65]{EinfBayesStatistik} \\\\
Die \textbf{quadratische Kostenfunktion} ist definiert als: $L(x-\hat{x}) = (x-\hat{x})\Sigma^{-1}(x-\hat{x})$. \\
Diese gibt den quadratischen Abstand zwischen dem wahren Parameter $x$ und der Schätzung $\hat{x}$ an.
Die zu erwartenden Kosten werden berechnet mit dem Erwartungswert der Kostenfunktion.
Diese Schätzung führt zu dem Erwartungswert von $x$, das heißt $\hat{x} = E(x)$. \parencite[65-66]{EinfBayesStatistik} \\\\
Die \textbf{Kostenfunktion der absoluten Fehler} ist definiert als: $L(x,\hat{x}) = |x-\hat{x}|$.
Diese gibt den absoluten Abstand zwischen $x$ und $\hat{x}$ an.
Die Schätzung mit dem absoluten Fehler ergibt den Median der Verteilung, das heißt $F(\hat{x}_{med}) = 0.5$ \parencite[67-68]{EinfBayesStatistik} \\\\\\\\
Die \textbf{Null-Eins-Kostenfunktion} bedeutet, dass es entweder Kosten oder keine Kosten gibt.
Diese ist definiert durch: $L(x-\hat{x}) = 
\begin{cases}
  0 & \text{für } |x-\hat{x}| < b \\
  a & \text{für } |x-\hat{x}| \geq b
\end{cases}$,\\
wobei $a$ und $b$ als Konstaten angenommen werden. Wenn der Fall $b \to 0$ betrachtet wird,
ergibt sich als Schätzer das Argument des Maximums der Posterioriverteilung, das heißt $\hat{x}_M = \arg \max \; p(x|y)$. \parencite[68-69]{EinfBayesStatistik}\\\\
Es ist so ersichtlich, dass es verschiedene Punktschätzer in der bayesianischen Statistik gibt:
\begin{itemize}
  \item \textbf{Erwartungswert} $\hat{x} = E(x)$
  \item \textbf{Median} $\hat{x} = x_{0.5}$
  \item \textbf{Maximum der Posterioriverteilung} $\hat{x} = \arg \max \; p(x|y)$
\end{itemize}
Die Wahl zwischen den Schätzern hängt so von der Problemstellung ab.

\paragraph{Konfidenzintervalle} \mbox{}\\\\
Um Unsicherheiten bei Schätzungen zu quantifizieren werden oftmals \textbf{Konfidenzintervall} verwendet. Konfidenzintervalle geben einen Bereich an, in dem ein unbekannter Parameter mit einer bestimmten Wahrscheinlichkeit liegt. In der Bayes Statistik kann eine Bereichschätzung unmittelbar aus der Posterioriverteilung abgeleitet werden. \parencite[71]{EinfBayesStatistik} \\\\
Wenn $P(x | y)$ die Posterioridichte für den Parameter x ist, ist das Konfidenzintervall mit Konfidenzniveau 1-$\alpha$ definiert als:
\begin{equation}
  P(x \in X_u|y) = \int_{X_u} P(x|y) dx = 1-\alpha
\end{equation}
Wobei für alle $x_1 \in X_u, x_2 \not\in X_u$ gilt $P(x_1|y) \geq P(x_2|y)$; Xu ist dabei ein Unterraum auf dem Parameterraum \parencite[71]{EinfBayesStatistik}. Meist wird für $\alpha$ 0.1, 0.05 oder 0.01 gewählt. Höhere Konfidenzniveaus führen zu größeren Intervallen, da mehr Unsicherheit abgedeckt wird, während niedrigere Konfidenzniveaus kleinere Intervalle liefern, die jedoch ein höheres Risiko haben, den wahren Parameter nicht einzuschließen. \\\\
Falls die Posterioriverteilung $P(x | y)$ beispielsweise der Normalverteilung $N(\mu,\sigma^2)$ ent- spricht, ist das Konfidenzintervall symmetrisch um den Mittelwert und die Grenzen lassen sich einfach durch die Quantile der Standardnormalverteilung z über $\mu - z_{1-\alpha/2} \cdot \sigma$ und $\mu + z_{1-\alpha/2} \cdot \sigma$ bestimmen.\\\\
Da die Konfidenzintervalle in der bayesianischen Statistik auf der Posterioriverteilung basieren, unterscheiden sich die Ergebnisse von denen der klassischen Statistik, da diese auf einer anderen Grundlage definiert sind.

\newpage
\paragraph{Hypothesenprüfung} \mbox{}\\\\
Hypothesenprüfung ist eine Methode, mit der man Annahmen über unbekannte Para-meter überprüft. Dabei wird entschieden, ob solche Annahmen akzeptiert oder abgelehnt werden sollen. \parencite[74]{EinfBayesStatistik}. Man unterscheidet zwischen der Nullhypothese $H_0$ und der Alternativhypothese $H_1$. Die Nullhypothese ist die Annahme, die überprüft wird, während die Alternativhypothese die Annahme ist, die akzeptiert wird, wenn die Nullhypothese abgelehnt wird. \\\\
In der bayesianischen Statistik werden Hypothesen durch Unterräume des Parameterraums definiert. Seien $X_1$ und $X_2$ disjunkte Unterräume des Parameterraums, so lauten die Hypothesen $H_0: x \in X_1$ und $H_1: x \in X_2$. Die Wahrscheinlichkeiten für diese Hypothesen ergeben sich direkt aus der Posterior-Dichte. Die Wahrscheinlichkeit, dass die Nullhypothese wahr ist, lautet:
\begin{equation}
  P(H_0|y) = \int_{X_1} P(x|y) dx
\end{equation}
Analog gilt dies für $P(H_1|y)$ \parencite[74]{EinfBayesStatistik}. Ist jetzt $P(H_0|y) > P(H_1|y)$ rsp. $\frac{P(H_0|y)}{P(H_1|y)}>1$, so wird die Nullhypothese akzeptiert, andernfalls die Alternativhypothese. \parencite[77]{EinfBayesStatistik}. \\\\
Bei einer Punkthypothese $H_0: x = x_0$ ergibt sich $P(H_0|y) = 0$, da die Posterior-Dichte kontinuierlich ist und ein einzelner Punkt daher keine Wahrscheinlichkeit besitzt. In solchen Fällen greift man beispielsweise auf Konfidenzintervalle zurück: Liegt der Wert $x_0$ innerhalb des Konfidenzintervalls, wird die Nullhypothese akzeptiert, andernfalls abgelehnt \parencite[84]{EinfBayesStatistik}. \\\\
Ein wesentlicher Unterschied zur klassischen Statistik besteht darin, dass in der traditionellen Hypothesenprüfung die Nullhypothese solange als gültig angesehen wird, bis genügend Beweise dagegen vorliegen (Prinzip des Zweifelns). In der bayesiani-schen Statistik hingegen werden die Wahrscheinlichkeiten beider Hypothesen direkt geschätzt, wodurch beide Hypothesen von Anfang an gleichberechtigt betrachtet werden. \parencite[83]{EinfBayesStatistik}.


\newpage

\subsection{Markov Chain Monte Carlo (MCMC) Methoden}
\subsubsection{Einführung in MCMC-Methoden}
Bei einer direkten Simulation wird vorrausgesetzt, dass die Verteilung der Zufallsvaria-blen bekannt ist.
Dies ist jedoch in der Praxis nicht immer gegeben.
Die Berechnung der Posterioriverteilung ist analytisch oft nicht möglich, vor allem bei komplexen Mo-dellen oder hohen Dimensionen. \\\\
Die Markov Chain Monte Carlo (MCMC) Methoden sind eine Klasse von Algorithmen, die es ermöglichen, eine Stichprobe aus einer Verteilung zu ziehen, ohne die Verteilung zu kennen. \parencite[179]{MonteCarloAlgorithmen} \\
Diese Methoden verwenden zwei Konzepte: Markov-Ketten und Monte Carlo-Methoden. \\
Eine Markov-Kette ist eine Folge von Zufallsvariablen, die die Markov-Eigenschaft erfüllen. 
Die Markov-Eigenschaft sagt aus, dass die nächste Zufallsvariable nicht von den vorherigen Zufallsvariablen, sondern nur von der letzten Zufallsvariable abhängt. 
Das bedeutet, dass die Wahrscheinlichkeit, im nächsten Zustand $X_n+1$ zu landen, nur von $X_n$ abhängt. 
Die Übergangswahrscheinlichkeit zwischen den Zuständen kann in einer Übergangsmatrix dargestellt werden. \parencite[188f.]{MonteCarloAlgorithmen} \\
Die Monte Carlo-Methoden sind eine Gruppe von Algorithmen, die es ermöglichen, Zufallsvariablen zu schätzen, indem Zufallszahlen generiert werden. 
Sie erzeugen zufällige Stichproben, um eine Näherung der Verteilung zu erhalten. \parencite[14f.]{MonteCarloAlgorithmen} \\\\
Die MCMC-Methoden nutzen die Monte Carlo-Methoden, um eine Markov-Kette zu simulieren. Diese Technik ist besonders nützlich, um eine Posterioriverteilung zu schätzen, wenn direkte Berechnungen nicht möglich sind. \parencite[179]{MonteCarloAlgorithmen} \\
Im folgenden wird der Metropolis-Hastings-Algorithmus erläutert, eine der bekann-testen MCMC-Methoden.
\newpage

\subsubsection{Der Metropolis-Hastings-Algorithmus}
Der Metropolis-Hastings-Algorithmus erstellt eine Markov-Kette, die eine Posteriori-verteilung $f(x)$ approximiert. $f(x)$ besteht aus einer bekannten Funktion $p(x)$, die sich aus der Likelihood-Funktion und der Prioriverteilung zusammensetzt und aus einer unbekannten Normierungskonstante. Für die Berechnung wird eine Hilfsfunktion q(y$|$x) benötigt, welche die Übergangswahrscheinlichkeit von einem Zustand $x_t$ zum nächsten Zustand $x_{\text{t+1}}$ angibt und ähnlich wie die Posterioriverteilung ist. Oftmals wird dafür die Normalverteilung gewählt sodass $q(y|x) = N(x,\sigma^2)$, wobei $\sigma$ beeinflusst dabei die Schrittweite, die die Markov-Kette macht. \parencite[226f.]{HandbookMonteCarloMethods} \\\\ 
Der Algorithmus besteht aus folgenden Schritten:
\begin{enumerate}
  \item Wähle einen Startwert $x_0$
  \item Wähle einen neuen Wert y aus der Hilfsfunktion $q(y|x_t)$ im Fall einer Normalverteilung $N(x_t,\sigma^2)$ wird ein zufälliger Wert aus der Normalverteilung gezogen
  \item Berechne die Akzeptanzwahrscheinlichkeit $\alpha$:
  \begin{equation}
    \alpha = min{\{1,\frac{p(y) \cdot q(x_t|y)}{p(x_t) \cdot q(y|x_t)}\}}
  \end{equation}
  Diese gibt an, wie wahrscheinlich der neue Wert $x_{\text{t+1}}$ akzeptiert wird.
  \item Ziehe eine Zufallszahl u aus einer Gleichverteilung $U(0,1)$
  \item Wähle den nächsten Wert $x_{\text{t+1}}$:
  \begin{equation}
    x_{t+1} = 
    \begin{cases}
      y, & \text{wenn u}< \alpha \\
      x_t, & \text{sonst}
    \end{cases}
  \end{equation}
  \item Wiederhole die Schritte 2-5 bis die Stichprobe die gewünschte Verteilung genau genug approximiert
\end{enumerate}
Der Metropolis-Hastings-Algorithmus ist ein Verallgemeinerung des Metropolis-Algo-rithmus, der nur für symmetrische Hilfsfunktionen $q(y|x)$ funktioniert. Bei einer symmetrischen Hilfsfunktion ist $q(y|x_t) = q(x_t|y)$ und deswegen kürzt sich die Akzeptanzwahrscheinlichkeit zu $\alpha = min{\{1,\frac{p(y)}{p(x_t)}\}}$. \parencite[226f.]{HandbookMonteCarloMethods} \\\\
Der erste Teil der Markov-Kette wird als \textbf{Burn-in-Phase} bezeichnet, welche verworfen wird, weil sich die Kette erst an die Zielverteilung anpassen muss. Der restliche Teil wird als \textbf{Sampling-Phase} bezeichnet, welche dann die Verteilung simuliert. \parencite[226f.]{HandbookMonteCarloMethods} \\\\

\newpage

\section{Praktischer Teil}

\subsection{Implementierung des Metropolis-Hastings-Algorithmus in Python}
Der Metropolis-Hasting-Algorithmus erstellt mit Hilfe der Posteriorifunktion eine Markov-Kette. Dafür wird die Normalverteilung als Hilfsfunktion verwendet, da sie ähnlich der Posteriorifunktion ist, welche in unserem Beispiel verwendet wird. Als $\mu$ wird der aktuelle Wert verwendet und als $\sigma^2$ wird 0.1 gewählt. Das $\sigma$ beeinflusst die Schrittweite, ist es hoch werden mit höherer Wahrscheinlichkeit vom aktuellen Wert weiter entfernte Werte gewählt. Da in unserem Beispiel eine Wahrscheinlichkeit geschätzt wird muss beachtet werden, dass neue Werte immer zwischen 0 und 1 liegen. Der folgende Python Code zeigt die Implementierung des Metropolis-Hastings-Algorithmus. 
\lstinputlisting[linerange={41-41,53-76}]{../src/binomStatistics.py}
\newpage

\subsection{Anwendung der bayesianischen Statistik am Beispiel}
\subsubsection{Überblick und Aufbereitung des Datensatzes}
Für den praktischen Teil der Arbeit nutzen wir einen CSV-Datensatz mit detatailierten Informationen zu Weltmeisterschaft-Fußballspielen. 
Dieser Datensatz wurde von der Plattform Kaggle heruntergeladen und ist unter folgendem Link zu finden: \url{https://www.kaggle.com/datasets/piterfm/fifa-football-world-cup}\parencite{FIFAWorldCupData}. \\\\
Der Datensatz enthält alle WM-Spiele zwischen 1930 und 2022 mit Informationen über die Manschaften, Ergebnisse, Austragungsorte, Spielereignisse (Auswechslungen etc.), Schiedrichterdaten und so weiter. Im ersten Teil wollen wir aber nur die Siegesquote der deutschen Nationalmannschaft analysieren. Dafür haben wir in \url{../src/world_cup_wins_germany.py} berechnet, wie viele Spiele die deutsche Nationalmannschaft gespielt und wie viele davon gewonnen hat. Die Anzahl gewonnener Spiele beträgt dabei 72 von insgesamt 112 Spielen. \\\\
Im späteren Verlauf unserer Untersuchungen wollen wir auch die Siegesquoten gegen europäische Teams mit denen gegen nicht-europäische Teams vergleichen. Dafür haben wir auch die Anzahl der gewonnenen Spiele gegen europäische und nicht-europäische Teams berechnet. Die Anzahl der gewonnenen Spiele gegen europäische Teams beträgt dabei 36 von insgesamt 65 Spielen und die Anzahl der gewonnenen Spiele gegen nicht-europäische Teams beträgt 36 von insgesamt 47 Spielen. \\\\

\subsubsection{Anwendung der bayesianischen Statistik}
Zuerst wollen wir die Siegesquote der deutschen Nationalmannschaft mit Hilfe des Metropolis-Hastings-Algorithmus schätzen. Als Vorwissen gehen wir dabei von einer um $\mu = 0.5$ normalverteilten Siegesquote mit einer Varianz von $\sigma^2 = 0.1$ aus, was somit unsere Prioiriverteilung ist. In die Likelihood Funktion fließt jetzt der gegebene Datensatz ein. Die Likelihood-Funktion ist somit eine Binomialverteilung mit den Parametern $x = 72$ und $n = 112$ ohne den Normierungsfaktor $\binom{n}{x}$. Die Posteriorifunktion ergibt sich aus der Multiplikation der Likelihood-Funktion und der Prioriverteilung. Dadurch ergeben sich folgende Python Funktionen, welche die Wahrscheinlichkeit für einen gegebenen Wert liefern.\\ 
\lstinputlisting[linerange={7-7,17-17,19-19,27-27,29-29,39-39}]{../src/binomStatistics.py}
\newpage
\noindent Ist n die Gesamtanzahl an Spielen und x die Anzahl an Siegen ist die Prioriverteilung $P(p)$ somit mathematisch definiert als:
\begin{equation}
  P(p) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left(-\frac{(p-\mu)^2}{2\sigma^2}\right)}
\end{equation}
Und die Likelihood-Funktion $P(n,x|p)$ als:
\begin{equation}
  P(n,x|p) = p^x(1-p)^{n-x}
\end{equation}
Um nun den Normierungsfaktor P(n,x) der Posterioriverteilung zu berechnen zu können müsste man folgendes Integral berechnen:
\begin{equation}
  P(n,x) = \int_{0}^{1} (p^x(1-p)^{n-x}) \cdot \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left(-\frac{(p-\mu)^2}{2\sigma^2}\right)} dp
\end{equation}
Da dieses Integral analytisch schwer zu lösen ist, können wir unseren Metropolis-Hastings-Algorithmus aus 3.1 verwenden, um eine Markovkette zu erstellen, die die Posterioriverteilung approximiert. Starten wir nun den Algorithums mit den Werten n = 112, x = 72 so erhalten wir folgende Posterioriverteilung:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../images/world_cup_germany.png}
  \caption{Posterioriverteilung der Siegesquote der deutschen Nationalmannschaft}
\end{figure}
\noindent In der Grafik sieht man, die zuvor angenommene Prioriverteilung und die Posterioriverteilung welche eine geringere Varianz besitzt und um einiges weiter rechts liegt als die Prioriverteilung. Es ist deutlich zu erkennen, dass die Siegesquote deutlich höher als die zuvor angenommenen 50\% liegt. Der Erwartungswert der Posterioriverteilung beträgt ungefähr 0.615.
\newpage
\noindent Wie stark die Posterioriverteilung nach rechts verschoben wird und wie stark die Varianz abnimmt, hängt von der Größe der Stichprobe ab. Je größer die Stichprobe, desto stärker wird die Posterioriverteilung verschoben und desto geringer wird die Varianz. Diesen Prozess haben wir durch \url{../src/showcases/binomStatisticsAnimated.py} simuliert und im Ergebniss ist die erwartete Verschiebung deutlich zu erkennen:
\begin{figure}[h]
  \centering
  % Erste Zeile mit zwei Bildern
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../images/framesAnimatedSampleSize/frame1.png}
      \caption{n=2}
      \label{fig:bild1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../images/framesAnimatedSampleSize/frame2.png}
      \caption{n=10}
      \label{fig:bild2}
  \end{subfigure}
  
  \vspace{0.5cm}
  
  % Zweite Zeile mit zwei Bildern
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../images/framesAnimatedSampleSize/frame3.png}
      \caption{n=50}
      \label{fig:bild3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../images/framesAnimatedSampleSize/frame4.png}
      \caption{n=200}
      \label{fig:bild4}
  \end{subfigure}
  
  \caption{Posterioriverteilung für verschiedene Stichprobengrößen}
  \label{fig:mainfig}
\end{figure}

\noindent Der Einfluss der Prioriverteilung auf die Posterioriverteilung nimmt mit steigender Stichprobengröße somit immer weiter ab. Das kann man auch gut in folgender Grafik erkennen, in welcher die Gleichverteilung auf [0,1] als Prioriverteilung verwendet wird. Es ist zu erkennen, dass die Posterioriverteilung sich immer weiter der oben abgebildeten Posterioriverteilung annähert also normalverteilt wird, obwohl die Prioriverteilung eine Gleichverteilung ist.
\newpage
\begin{figure}[h]
  \centering
  % Erste Zeile mit zwei Bildern
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../images/framesAnimatedSampleSizeUniform/frame1.png}
      \caption{n=2}
      \label{fig:bild1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{../images/framesAnimatedSampleSizeUniform/frame2.png}
      \caption{n=200}
      \label{fig:bild2}
  \end{subfigure}
  \caption{Posterioriverteilung für verschiedene Stichprobengrößen mit Gleichverteilung als Prioriverteilung}
\end{figure}

\newpage
\subsubsection{Statistische Auswertung der Ergebnisse}

Im weiteren Wollen wir nun noch die Konfidenzintervalle und den Punktschätzer der Markovkette berechnen und diese mit den Ergebnissen der klassischen Statistik verglei-chen. Wir gehen dabei wieder von der Stichprobengröße n=112 aus. Für das Konfidenzintervall in der klassischen Statistik nutzen wir das exakte Konfidenzintervall für eine unbekannte Wahrscheinlichkeit p. Dieses ist definiert als:
\begin{equation}
  I_e = \left[\frac{S_nF_1}{n-S_n+1+S_nF_1} , \frac{(S_n+1)F_2}{n-S_n+(S_n+1)F_2}\right]
\end{equation}
wobei $F_1$ das Quantil der F-Verteilung mit $\alpha/2$ und den Freiheitsgraden $2S_n$ und $2(n-S_n+1)$ ist und $F_2$ das Quantil der F-Verteilung mit 1-$\alpha/2$ und den Freiheitsgraden $2(S_n+1)$ und $2(n-S_n)$ ist. Sn ist dabei die Anzahl der gewonnenen Spiele. Für die Berechnung haben wir das folgende Python-Programm geschrieben:
\lstinputlisting[firstline=87,lastline=97]{../src/binomStatistics.py}

\noindent Dieses liefert den Punktschätzer 0.617 und das Konfidenzintervall [0.532, 0.7] mit der bayesianischen Statistik und 0.643 und [0.547,731] für die klassische Statistik. \\\\
Es fällt auf, dass in der klassischen Statistik die geschätzen Werte für p höher sind als mit der Bayes'schen Statistik. Das liegt ganz einfach daran, dass die Verteilung in der bayesianischen Statistik durch die Prioriverteilung noch leicht nach in Richtung 0.5 gezogen wird. Man kann somit gut sehen, dass sich die Werte je nach Berechnung unterscheiden können und die Ergebnisse mit klassischer und bayesianischer Statistik nicht die gleichen sein müssen. Der große Vorteil der bayesianischen Statistik ist aber, dass man viel mehr Informationen über die Schätzung von p in Form der Posteriori-verteilung erhält.

\newpage
\subsubsection{Rekursives Einbeziehen von neuen Daten}

Weiter wollen wir überprüfen, wie sehr sich die Posterioriverteilungen unterscheiden, wenn man immer wieder neue Daten erhält. Die erste Möglichkeit wäre, dass man seine Posterioriverteilung als Prioiriverteilung verwendet und so eine neue Posteriori-verteilung mit den neuen Daten als Likelihood berechnen kann. Eine weitere Möglichkeit ist es nochmal eine neue Posterioriverteilung zu berechnen mit der schon davor verwendeten Prioiriverteilung und der aktualisierten Stichprobe als Likelihood. Dafür haben wir das Python-Programm \url{../src/showcases/binomStatisticsRecursiv.py} erstellt, welche die beiden Varianten darstellt. Folgende Funktion simuliert die Prio-riverteilung mit der Stichprobe der vorherigen Posterioriverteilung:
\lstinputlisting[linerange={83-83,92-95}]{../src/showcases/binomStatisticsRecursiv.py}
\noindent Da es sich um eine Stichprobe handelt wird die Schätzung pro Iteration immer ungenauer, da man nicht die genaue Verteilung kennst. Ansonsten sieht man aber nicht so große Unterschiede zwischen den beiden Varianten, wie in der folgenden Grafik zu sehen ist:
\begin{figure}[h]
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{../images/framesAnimatedRecursive/frame1.png}
    \caption{Posteriori als Priori}
    \label{fig:bild5}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{../images/framesAnimatedRecursive/frame2.png}
    \caption{Zusammengefasste Stichprobe}
    \label{fig:bild6}
  \end{subfigure}

  \caption{Vergleich der 2 Varianten}
  \label{fig:mainfig}
\end{figure}
\newpage

\subsection{Berechnungen für den Vergleich der Siegesrate gegen europäische und nicht-europäische Teams}
\label{subsec:analyse}
Nachfolgend wollen wir die Siegesrate der deutschen Nationalmannschaft gegen europäische Teams mit der gegen nicht-europäische Teams vergleichen.
Insbesondere wollen wir die Frage beantworten, ob die Siegesrate gegen europäische Teams signifikant niedriger ist als gegen nicht-europäische Teams.
Für die nachfolgenden Berechnungen definieren wir somit:
\begin{itemize}
  \item \( \pi_E \) als die Siegesrate der deutschen Nationalmannschaft gegen europäische Teams
  \item \( \pi_{\neg E} \) als die Siegesrate der deutschen Nationalmannschaft gegen nicht-europäische Teams
  \item \( \pi_G \) als die Siegesrate der deutschen Nationalmannschaft insgesamt
  \item \( H_0 : \pi_E \geq \pi_{\neg E} \) gegen \( H_1 : \pi_E < \pi_{\neg E} \)
\end{itemize}
Dabei gehen wir die Fragestellung mit klassischer sowie mit bayesianischer Statistik an. \\

Daten:
\begin{itemize}
  \item Gegen europäische Teams wurden 65 Spiele gespielt, davon 36 gewonnen.
  \item Gegen nicht-europäische Teamsn wurden 47 Spiele gespielt, davon 36 gewonnen.
  \item Insgesamt wurden 112 Spiele gespielt, davon 72 gewonnen.
\end{itemize}

\subsubsection{Berechnung mit klassischer Statistik}

\textbf{Punktschätzer:}
\[
\hat{\pi}_E = \frac{36}{65} = 0.5538
\]
\[
\hat{\pi}_{\neg E} = \frac{36}{47} = 0.7659
\]
\[
\hat{\pi}_G = \frac{72}{112} = 0.6429
\]
\textbf{Konfidenzintervall:}
Für \(\alpha = 0.05\) beidseitige Konfidenzintervalle:

\[
I_E = \left[ 0.5538 - 1.96 \cdot \sqrt{\frac{0.5538 \cdot (1-0.5538)}{65}}, \, 0.5538 + 1.96 \cdot \sqrt{\frac{0.5538 \cdot (1-0.5538)}{65}} \right]
\]
\[
I_E = [0.4330, 0.6746]
\]

\[
I_{\neg E} = \left[ 0.7659 - 1.96 \cdot \sqrt{\frac{0.7659 \cdot (1-0.7659)}{47}}, \, 0.7659 + 1.96 \cdot \sqrt{\frac{0.7659 \cdot (1-0.7659)}{47}} \right]
\]
\[
I_{\neg E} = [0.6448, 0.8870]
\]

\[
I_G = \left[ 0.6429 - 1.96 \cdot \sqrt{\frac{0.6429 \cdot (1-0.6429)}{112}}, \, 0.6429 + 1.96 \cdot \sqrt{\frac{0.6429 \cdot (1-0.6429)}{112}} \right]
\]
\[
I_G = [0.5542, 0.7316]
\]
\textbf{Hypothesentest:} \\
Es wird ein einseitiger z-Test der Differenz der Anteile durchgeführt. \\

Wir berechnen den Standardfehler für den Unterschied der Anteile:
\[
SE = \sqrt{\hat{\pi}_G(1 - \hat{\pi}_G) \left(\frac{1}{65} + \frac{1}{47}\right)} = \sqrt{0.6429 \cdot (1 - 0.6429) \left(\frac{1}{65} + \frac{1}{47}\right)} \approx 0.0917
\]
Berechnung des z-Werts:
\[
z = \frac{\hat{\pi}_E - \hat{\pi}_{\neg E}}{SE} = \frac{0.5538 - 0.7659}{0.0917} \approx -2.31
\]
Für ein Signifikanzniveau von \( \alpha = 0.05 \) und einen einseitigen Test ist der kritische z-Wert:
\[
z_{\text{kritisch}} = -1.645
\]
Da der berechnete z-Wert von \(-2.31\) kleiner als der kritische z-Wert von \(-1.645\) ist, können wir die Nullhypothese \( H_0 : \pi_E \geq \pi_{\neg E} \) ablehnen.\\
Interpretation: Die Siegrate der deutschen Nationalmannschaft gegen europäische Teams ist signifikant niedriger als gegen nicht-europäische Teams.

\subsubsection{Berechnung mit bayesianischer Statistik}

\textbf{Vorwissen:}
Für die bayesianische Analyse nehmen wir als Prior-Verteilungen Beta-Verteilungen für die Siegquote der deutschen Nationalmannschaft gegen europäische und nicht-europäische Teams an. Die Parameter dieser Verteilungen basieren auf den beobachteten Daten.

\begin{itemize}
  \item Für europäische Teams:
    \[
    \alpha_E = 36 + 1 = 37, \quad \beta_E = 29 + 1 = 30
    \]
    Somit ist die Prior-Verteilung: \( \text{Beta}(37, 30) \).

  \item Für nicht-europäische Teams:
    \[
    \alpha_{\neg E} = 36 + 1 = 37, \quad \beta_{\neg E} = 11 + 1 = 12
    \]
\end{itemize}
Somit ist die Prior-Verteilung: \( \text{Beta}(37, 12) \).\\\\
\textbf{Likelihood-Verteilung:} \\
Für die gesamte Siegquote (Likelihood) wird ebenfalls eine Beta-Verteilung verwendet, basierend auf den gesamten Spielen der deutschen Nationalmannschaft:
\[
\alpha_{\text{total}} = 72 + 1 = 73, \quad \beta_{\text{total}} = 40 + 1 = 41
\]
Die Likelihood-Verteilung ist daher: \( \text{Beta}(73, 41) \).\\\\
\textbf{Posterior-Verteilung:} \\
Die Posterior-Verteilung kann als Produkt von Prior und Likelihood berechnet werden. Sie wird durch Normierung auf den gesamten Bereich der Siegquote \( p \) bestimmt. \\\\
Die Verteilungen für die Prior-, Likelihood- und Posterior-Verteilungen werden wie folgt dargestellt:

\begin{itemize}
  \item Prior für europäische Teams: \( \text{Beta}(37, 30) \)
  \item Prior für nicht-europäische Teams: \( \text{Beta}(37, 12) \)
  \item Likelihood für alle Spiele: \( \text{Beta}(73, 41) \)
\end{itemize}
Die Posterior-Verteilungen für europäische und nicht-europäische Teams werden durch das Produkt der Prior- und Likelihood-Verteilungen berechnet.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{../images/world_cup.png}
  \caption{Prior-, Likelihood- und Posterior-Verteilungen der Siegquote der deutschen Nationalmannschaft}
\end{figure}
\noindent
\textbf{Punktschätzer:} \\
Der Erwartungswert der Posterior-Verteilung ist der Mittelwert der Beta-Verteilung, der mit folgender Formel berechnet wird:
\[
E = \frac{\alpha_{\text{posterior}}}{\alpha_{\text{posterior}} + \beta_{\text{posterior}}}
\]
Für europäische Teams ist der Erwartungswert:
\[
E_E = \frac{37 + 73}{37 + 73 + 30 + 41} = \frac{110}{181} \approx 0.6077
\]
Für nicht-europäische Teams ist der Erwartungswert:
\[
E_{\neg E}= \frac{37 + 73}{37 + 36 + 12 + 41} = \frac{110}{163} \approx 0.6748
\]
\textbf{Konfidenzintervall:} \\
Das 95\%-Konfidenzintervall für die Posterior-Verteilung wird durch die Quantilfunktion der Beta-Verteilung berechnet. Wir verwenden das 2.5\%- und 97.5\%-Quantil.
Dies wurde mithilfe des Programms \texttt{world\_cup\_bayes.py} durchgeführt. \\
Für europäische Teams:

\[
I_E = [0.5358, 0.6775]
\]
Für nicht-europäische Teams:
\[
I_{\neg E} = [0.6012, 0.7444]
\]
\textbf{Hypothesentest:} \\
Um die Hypothese \(H_0\) zu testen, dass die Siegquote gegen europäische Teams niedriger ist als gegen nicht-europäische Teams, wurden Monte-Carlo-Simulationen verwendet:

\begin{itemize}
    \item Aus den Posterior-Verteilungen wurden \( 100{.}000 \) Zufallswerte (Samples) gezogen.
    \item Die Wahrscheinlichkeit \( P(H_1) = P(\pi_E < \pi_{\neg E}) \) wurde als der Anteil der Simulationen berechnet, bei denen \( \pi_E < \pi_{\neg E} \).
    \item Die Wahrscheinlichkeit \( P(H_0) = 1 - P(H_1) \) wurde ebenfalls berechnet.
    \item Je nachdem, welche Wahrscheinlichkeit größer ist, wurde entschieden, ob \( H_0 \) (Nullhypothese) oder \( H_1 \) (Alternativhypothese) akzeptiert wird.
\end{itemize}
\textbf{Ergebnisse:} \\
\( P(H_0) = 0.0972 \) und \( P(H_1) = 0.9028 \) \\
Da \( P(H_1) > P(H_0) \), wird die Nullhypothese \( H_0 : \pi_E \geq \pi_{\neg E} \) abgelehnt. \\
Interpretation: Die Siegquote der deutschen Nationalmannschaft gegen europäische Teams ist signifikant niedriger als gegen nicht-europäische Teams.
\newpage


\section{Vergleich und Fazit}


\subsection{Vergleich der klassischen und bayesianischen Statistik}

\begin{table}[h!]
  \centering
  \begin{tabular}{|p{3cm}|p{5cm}|p{6cm}|}
    \hline
    \textbf{} & \textbf{Klassische Statistik} & \textbf{Bayesianische Statistik} \\ \hline
    \textbf{Bevorzugte Verwendung} & Human- und Sozialwissenschaften, Wirtschaftswissenschaften, Biologie & Technik und Künstliche Intelligenz \\ 
    \hline
    \textbf{Schätzen von Parametern, Testen von Hypothesen} & nur Stichprobe wird betrachtet & Stichprobe und Vorwissen wird betrachtet \\
    \hline
    \textbf{Ergebnis beim Schätzen} & Punkt- oder Intervallschätzer, z. B. Konfidenzintervall & Wahrscheinlichkeitsverteilung für den Parameter (Posterior) 
    \\ \hline
  \textbf{Ergebnis beim Testen} & \(p\)-Wert; Entscheidung basierend auf Signifikanzniveau, wird angenommen oder abgelehnt & Posterior-Wahrscheinlichkeit für Hypothesen \\ 
  \hline
  \end{tabular}
  \caption{Vergleich der klassischen und bayesianischen Statistik}
  \label{tab:vergleich_verwendung}
\end{table}
\noindent
Die klassische Statistik und die bayesianische Statistik sind zwei unterschiedliche Ansätze zur Analyse
von Daten und zur Entscheidungsfindung. Während die klassische Statistik vor allem in den Human- und 
Sozialwissenschaften sowie in der Wirtschaftswissenschaft und Biologie weit verbreitet ist, wird die 
bayesianische Statistik häufig in der Technik und im Bereich der Künstlichen Intelligenz eingesetzt.\\\\
Ein wesentlicher Unterschied zwischen den beiden Methoden liegt in der Art und Weise, wie Parameter 
geschätzt und Hypothesen getestet werden. Die klassische Statistik betrachtet dabei ausschließlich 
die vorliegende Stichprobe, während die bayesianische Statistik sowohl die Stichprobe als 
auch vorheriges Wissen in die Analyse einbezieht. \\\\
Auch die Ergebnisse unterscheiden sich: In der klassischen Statistik erfolgt die Schätzung von 
Parametern durch Punkt- oder Intervallschätzer, beispielsweise in Form eines Konfidenzintervalls. 
Die bayesianische Statistik hingegen liefert eine Wahrscheinlichkeits-verteilung für den 
gesuchten Parameter, die sogenannte Posterior-Verteilung. \\\\
Beim Testen von Hypothesen arbeitet die klassische Statistik mit dem p-Wert und einer Entscheidungsregel 
basierend auf einem festgelegten Signifikanzniveau. Eine Hypothese wird entweder angenommen oder abgelehnt. 
Die bayesianische Statistik hingegen bewertet Hypothesen anhand der Posterior-Wahrscheinlichkeit und verwendet 
den Bayes-Faktor zur Entscheidungsfindung, wodurch ein flexiblerer und probabilistischerer Ansatz ermöglicht wird.

\newpage
\begin{table}[h!]
  \centering
  \begin{tabular}{|p{4cm}|p{5cm}|p{5cm}|}
  \hline
  \textbf{Statistikmethode} & \textbf{Vorteile} & \textbf{Nachteile} \\ \hline
  \textbf{Klassische Statistik} & 
  \begin{itemize}
      \item Einfachheit
      \item Gut für große Datensätze
      \item keine subjektiven Annahmen
  \end{itemize} & 
  \begin{itemize}
      \item abhängig von großen Stichproben
      \item unflexibel gegenüber neuen Daten oder komplexen Datenstrukturen
  \end{itemize} \\ \hline
  \textbf{Bayesianische Statistik} & 
  \begin{itemize}
      \item flexibel und anpassbar an komplexe Modelle.
      \item ermöglicht die Einbeziehung von Vorwissen
      \item liefert direkte Wahrscheinlichkeiten für Parameter und Modelle
  \end{itemize} & 
  \begin{itemize}
      \item Subjektivität bei der Wahl der Priors.
      \item rechenintensiv, besonders bei großen Datensätzen.
  \end{itemize} \\ \hline
  \end{tabular}
  \caption{Vergleich der Vor- und Nachteile der klassischen und bayesianischen Statistik.}
  \label{tab:vergleich_vor_nachteile}
\end{table}
\noindent
Die klassische (frequentistische) Statistik ist einfach anzuwenden und weit verbreitet, da 
sie auf standardisierten Testverfahren basiert. Im Gegensatz dazu ist die bayesianische Statistik 
komplexer und erfordert einen höheren Rechenaufwand, insbesondere bei großen Datensätzen. \\\\
Ein Vorteil der bayesianischen Statistik ist ihre Flexibilität und die Möglichkeit, Vorwissen 
in Form von Priors einzubeziehen. Dagegen ist die klassische Statistik unflexibel gegenüber 
neuen Daten und Modellen, da sie stark von großen Stichproben abhängt. \\\\
Die klassische Statistik benötigt keine subjektiven Annahmen über Wahrscheinlichkeiten, wodurch 
sie als objektiver gilt. Demgegenüber steht die bayesianische Statistik, bei der die Wahl der 
Priors subjektiv sein kann und dadurch die Ergebnisse beeinflusst. \\\\
Während die klassische Statistik gut für große Datensätze geeignet ist, können ihre Ergebnisse 
bei komplexen Datenstrukturen schwer interpretierbar sein. Die bayesianische Statistik hingegen 
liefert direkte Wahrscheinlichkeiten für Parameter und Modelle, 
was eine intuitivere Interpretation ermöglicht. \\\\
Zusammenfassend bietet die klassische Statistik eine einfache und weithin akzeptierte Methode 
zur Datenanalyse, ist aber in ihrer Anpassungsfähigkeit begrenzt. Die bayesianische Statistik 
ist flexibler und anpassungsfähiger, erfordert jedoch mehr Rechenleistung und die sorgfältige 
Wahl von Priors. Die Entscheidung für eine Methode sollte daher je nach Anwendungsfall getroffen werden.

\newpage

\section{Benutzerdokumentation}
\subsection{Voraussetzungen}
Für die Ausführung der Python-Programme wurde Version 3.12 verwendet. Zusätzlich sind die folgenden Bibliotheken erforderlich:
\begin{itemize}
  \item numpy
  \item matplotlib
  \item scipy
  \item sys
  \item csv
\end{itemize}

\subsection{Starten der Programme}
Zum Ausführen der Programme muss man sich im richtigen Verzeichnis befinden. Die Programme \texttt{binomStatistics.py}, \texttt{worldCupWinsGermany.py} und \texttt{worldCupBayes.py} befinden sich im Verzeichnis \texttt{src}. Alle übrigen Programme müssen aus dem Verzeichnis \texttt{src/showcases} ausgeführt werden. Die Programme werden über die Konsole mit den folgenden Befehlen gestartet.

\paragraph{binomStatistics.py} \mbox{}\\\\
Dieses Programm ist unserer Hauptprogramm, welches mit Eingabe einer Stichprobe die Posterioriverteilung berechnet und weitere Auswertungen ausgibt. Als Priorverteilung wird die Normalverteilung mit $\mu = 0.5$ und $\sigma^2 = 0.1$ verwendet. Die Ausführung erfolgt mit folgendem Befehl:
\begin{lstlisting}
  python binomStatistics.py <Anzahl der Versuche> <Anzahl Erfolge>
\end{lstlisting}

\paragraph{worldCupWinsGermany.py} \mbox{}\\\\
Dieses Programm analysiert die bereitgestellten CSV-Daten und berechnet die Anzahl der von der deutschen Nationalmannschaft gewonnenen Spiele. Es wird mit folgendem Befehl gestartet:
\begin{lstlisting}
  python worldCupWinsGermany.py
\end{lstlisting}

\paragraph{worldCupBayes.py} \mbox{}\\\\
Dieses Programm führt die in \autoref{subsec:analyse} beschriebenen Berechnungen durch.
\begin{lstlisting}
  python worldCupBayes.py
\end{lstlisting}

\paragraph{binomStatisticsUniform.py/binomStatisticsAnimated.py} \mbox{}\\\\
Diese beiden Programme demonstrieren den Einfluss der Stichprobengröße auf die Posteriorverteilung:

\begin{itemize}
\item \texttt{binomStatisticsUniform.py} verwendet eine Gleichverteilung als Priorverteilung.
\item \texttt{binomStatisticsAnimated.py} nutzt eine Normalverteilung als Priorverteilung.
\end{itemize}

Die Programme werden mit folgendem Befehl gestartet:
\begin{lstlisting}
  python binomStatisticsUniform.py <Ratio der Erfolge zu Gesamtversuchen>
\end{lstlisting}

\paragraph{binomStatisticsRecursiv.py} \mbox{}\\\\
Dieses Programm demonstriert das rekursive Einbeziehen neuer Daten in die Analyse. Die Posteriorverteilung der ersten Stichprobe wird als Priorverteilung für die Berechnung der Posteriorverteilung der nächsten Stichprobe genutzt. Anschließend wird diese resultierende Posteriorverteilung mit einer Posteriorverteilung verglichen, die beide Stichproben zusammenfasst. Die Ausführung erfolgt durch den Befehl:
\begin{lstlisting}
  python binomStatisticsRecursiv.py
\end{lstlisting}

\newpage

\section{Literaturverzeichnis} 

\printbibliography

\end{document}